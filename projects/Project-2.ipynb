{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project-2-Linear-Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nS8-e033n6B"
      },
      "source": [
        "# Project 2: Linear Regression\n",
        "\n",
        "Complete each of the exercises below. Write your code only in the cells containing the message:\n",
        "\n",
        "<font color='green'>############ Your code goes here ##################\n",
        "\n",
        "###################################################\n",
        "</font>\n",
        "  \n",
        "Do **NOT** change any other code. You can create new code cells to try things however before you submit the project please delete them. \n",
        "\n",
        "You are provided test functions for some of the exercises. In order to be able to run the tests you need to run the code cell below everytime you open this notebook or reset the runtime. \n",
        "\n",
        "In this project you will be implementing the linear regression algorithm.\n",
        "\n",
        "Run the following code cell to import the necessary utility and test functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PShFsp3HxL"
      },
      "source": [
        "import os \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if not os.path.isdir('MATH448001'):\n",
        "    !git clone https://github.com/iuls/MATH448001.git\n",
        "\n",
        "from MATH448001.project_tests import project_2_tests\n",
        "from MATH448001.project_utils import project_2_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afQoEG0TiEo9"
      },
      "source": [
        "# Exercise 1 (10 points)\n",
        "\n",
        "The weights of a neural network are randomly initialized at the beginning of training. There are different ways to initialize weights. One popular method is called `GlorotUniform` and it initializes a weight matrix of shape $(n_{in}, n_{out})$ by sampling from a uniform distribution over $[-l, l]$ where \n",
        "$$\n",
        "l = \\sqrt{\\frac{6}{n_{in}+n_{out}}}.\n",
        "$$\n",
        "\n",
        "Implement a function that takes integers `n_in, n_out` as its arguments and returns a pair $W$ and $b$ where \n",
        " - $W$, the weights, is a matrix of shape `(n_in, n_out)` sampled from the uniform distribution described above and \n",
        " - $b$, the biases, is a zero matrix of shape `(1, n_out)`.\n",
        "\n",
        "Hint: [`np.random.uniform`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html). Don't worry about the fact that this function samples from a half open interval. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXrWNvXBqa16"
      },
      "source": [
        "def initialize_weights_and_biases(n_in, n_out):\n",
        "    ''' Initializes the weights and biases\n",
        "    Arguments:\n",
        "    n_in: int, number of input units\n",
        "    n_out: int, number of output units\n",
        "\n",
        "    Returns:\n",
        "    W: The weights, a numpy array of shape (n_in, n_out)\n",
        "    b: The biases, a numpy array of shape (1, n_out)\n",
        "    '''\n",
        "    \n",
        "    np.random.seed(seed = 1)\n",
        "    \n",
        "    ########## Your Code goes here #############\n",
        "\n",
        "    \n",
        "    ############################################\n",
        "    \n",
        "    return W, b"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoBwRnHWYOpt"
      },
      "source": [
        "project_2_tests.test_initialize_weights_and_biases(initialize_weights_and_biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Greut36pqDRa"
      },
      "source": [
        "# Exercise 2 (10 points)\n",
        "\n",
        "**Forward pass**\n",
        "\n",
        "The linear regression model is shown in the following figure:\n",
        "\n",
        "![linear regression](https://drive.google.com/uc?export=view&id=1ylxsSWZ_qGMNtooA0-kgcRigBF-PYA1W)\n",
        "\n",
        "The forward pass of a given input $x = [x_1, \\ldots, x_n]$ is\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\hat{y} & = & f_{w,b}(x) \\\\\n",
        "& = & x \\cdot w + b \\\\ \n",
        "& = & x_1w_1 + \\ldots + x_nw_n + b \\\\\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "\n",
        "Implement the `forward_pass` function below that takes inputs `x`, `weights`, and `biases` as its arguments and returns the prediction $\\hat{y}$. Note that your function should be able to work for multiple inputs. For example `x` can be a matrix where each row is a particular data point:\n",
        "$$\n",
        "X = \\begin{bmatrix} x^{(1)}_1 & \\ldots & x^{(1)}_n \\\\ \\vdots & \\ddots & \\vdots \\\\ x^{(m)}_1 & \\ldots & x^{(m)}_n\\end{bmatrix}.\n",
        "$$\n",
        "In this case the output should be \n",
        "$$\n",
        "\\hat{y} = \\begin{bmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix} = X \\cdot w + b.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk53o38wPbv1"
      },
      "source": [
        "def forward_pass(x, weights, biases):\n",
        "    ''' Computes the predictions for a given set of data points\n",
        "    Arguments: \n",
        "    x : a numpy array of shape (m, n)\n",
        "    weights: the weight vector, a numpy array of shape (n,1)\n",
        "    biases: the bias, a numpy array of shape (1,1)\n",
        "\n",
        "    Returns:\n",
        "    y_hat: the predictions, a numpy array of shape (m,1)\n",
        "    '''\n",
        "    ################### Your code goes here ################\n",
        "    \n",
        "\n",
        "    ########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wI35vNXfXk4U"
      },
      "source": [
        "project_2_tests.test_forward_pass_linear_regression(forward_pass)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBgfGE4_alWN"
      },
      "source": [
        "# Exercise 3 (10 points)\n",
        "\n",
        "In this exercise, you will implement the mean squared error function. Recall that for a given set of data points $\\mathcal{D} = \\{(x^{(i)}, y^{(i)})\\}_{1\\leq i \\leq m}$, the mean squared error as a function of the parameters $w, b$, and $\\mathcal{D}$ is defined as \n",
        "$$\n",
        "\\mathcal{E}(f_{w,b}, \\mathcal{D}) = \\frac{1}{m}\\sum_{i=1}^m (f_{w,b}(x^{(i)}) - y^{(i)})^2 = \\frac{1}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})^2\n",
        "$$\n",
        "The function below takes two arguments:\n",
        " - `y_hat`: an array of predictions $[\\hat{y}^{(1)}, \\ldots, \\hat{y}^{(m)}]$ corresponding to the input values $[x^{(1)}, \\ldots, x^{(m)}]$. \n",
        " - `y`: the labels corresponding to the same input values.  \n",
        "\n",
        "and returns the mean squared error computed using the definition above. \n",
        "\n",
        "Implement the `mean_squared_error` function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrMN3aqmDn3D"
      },
      "source": [
        "def mean_squared_error(y_hat, y):\n",
        "    ''' Computes mean squared error\n",
        "    Arguments:\n",
        "    y_hat: a numpy array of shape (m,1)\n",
        "    y : a numpy array of shape (m,1)\n",
        "\n",
        "    Returns:\n",
        "    loss: The mean squared error.\n",
        "    '''\n",
        "\n",
        "    ################### Your code goes here ################\n",
        "\n",
        "\n",
        "    ########################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxsjzJzY66Ga"
      },
      "source": [
        "project_2_tests.test_mse(mean_squared_error)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McnPJWY7vApI"
      },
      "source": [
        "# Exercise 4 (20 points)\n",
        "\n",
        "**Find gradients and update parameters**\n",
        "\n",
        "In this step you will implement a function which updates the weights and biases according to the gradient of the loss function calculated for a batch of $m$ data points. In machine learning lingo this step is called 'backpropagation'.\n",
        " \n",
        "In order to update $w_k$, we first need to find the derivative of $\\mathcal{E}$ with respect to $w_k$:\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial \\mathcal{E}}{\\partial w_k} & = & \\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial }{\\partial w_k} (\\hat{y}^{(i)} - y^{(i)})^2\\\\\n",
        "& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) \\frac{\\partial \\hat{y}^{(i)}}{\\partial w_k} \\\\\n",
        "& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}_k\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "Similarly the derivatives with respect to $b$:\n",
        "\\begin{eqnarray}\n",
        "\\frac{\\partial \\mathcal{E}}{\\partial b} & = & \\frac{1}{m}\\sum_{i=1}^m\\frac{\\partial }{\\partial b} (\\hat{y}^{(i)} - y^{(i)})^2\\\\\n",
        "& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) \\frac{\\partial \\hat{y}^{(i)}}{\\partial b} \\\\\n",
        "& = & \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n",
        "\\end{eqnarray}\n",
        "\n",
        "Now given a mini-batch $\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m$, the update equations for the weights are\n",
        "$$\n",
        "\\begin{eqnarray}\n",
        "w_k^{new} & = & w_k^{old} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial w_k} = w_k^{old} - \\alpha \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)}) x^{(i)}_k \\\\\n",
        "b^{new} & = & b^{old} - \\alpha \\frac{\\partial \\mathcal{E}}{\\partial b} = b^{old} - \\alpha \\frac{2}{m}\\sum_{i=1}^m (\\hat{y}^{(i)} - y^{(i)})\n",
        "\\end{eqnarray}\n",
        "$$\n",
        "Here $\\alpha$ is the learning rate.\n",
        "\n",
        "Implement a function that computes these derivatives and updates the weight and bias accordingly. The arguments of the function are `x`, `y`, `weights`, `biases`, and `learning_rate`. Think of $x$ as\n",
        "$$\n",
        "x = \\begin{bmatrix} x^{(1)}_1 & \\ldots & x^{(1)}_n \\\\ \\vdots & \\ddots & \\vdots \\\\ x^{(m)}_1 & \\ldots & x^{(m)}_n\\end{bmatrix}\n",
        "$$\n",
        "and $y$ as \n",
        "$$\n",
        "y = \\begin{bmatrix} y^{(1)} \\\\ \\vdots \\\\ y^{(m)} \\end{bmatrix} \n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEUX22bqPj17"
      },
      "source": [
        "def update_parameters(x, y, weights, biases, learning_rate):\n",
        "    ''' Updates parameters according to gradients.\n",
        "    Arguments:\n",
        "    x: A numpy array of shape (m,n) where each row corresponding to a data point\n",
        "    y: A numpy array of shape (m,1) where each number is the label for the corresponding\n",
        "        row in x\n",
        "    weights: The weight vector, a numpy array of shape (n,1)\n",
        "    biases: The bias value, a numpy array of shape (1,1)\n",
        "\n",
        "    Returns: \n",
        "    weights: The updated weight vector, a numpy array of shape (n,1)\n",
        "    biases: The updated bias value, a numpy array of shape (1,1) \n",
        "    '''\n",
        "    \n",
        "    ################### Your code goes here ################\n",
        "    \n",
        "\n",
        "    \n",
        "    ########################################################\n",
        "    \n",
        "    return weights, biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJ69EBpdm8n7"
      },
      "source": [
        "project_2_tests.test_update_parameters_linear_reg(update_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "473coDUlGY6F"
      },
      "source": [
        "# Exercise 5: (0 points)\n",
        "\n",
        "Now let's load some data. Run the following cell to load training and test data. We will use the training data to train the model and then use the test data to test its performance. Since the model did not train on the test data, the performance on the test data is an indication of the performance of the model on unseen data. \n",
        "\n",
        "(To be able to create visualizations, the input features and labels were chosen to be one dimensional.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8vbJBGKWFCs"
      },
      "source": [
        "x_train, y_train, x_test, y_test = project_2_utils.load_regression_data()\n",
        "project_2_utils.plot_regression_data(x_train, y_train)\n",
        "project_2_utils.plot_regression_data(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSi5pdeGIWKO"
      },
      "source": [
        "The dataset consists of 100 training samples and 100 test samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVXb8e4PIeZy"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ZAOYwTH2lm"
      },
      "source": [
        "# Exercise 6 (30 points)\n",
        "\n",
        "Now let's train the linear regression model on this dataset. \n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "Set hyperparameters `batch_size` and `learning_rate`. Try different values of these parameters to get a validation loss less than 0.95.\n",
        "\n",
        "`batch_size` is the number of training data points to be used for each gradient descent update (i.e., the variable $m$ in Exercise 4 above). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xESGWfLPDRN3"
      },
      "source": [
        "################### Your code goes here ################\n",
        "batch_size = \n",
        "learning_rate = \n",
        "########################################################\n",
        "\n",
        "# Don't change the epochs\n",
        "epochs = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDd-Kj4drCiD"
      },
      "source": [
        "**Train**\n",
        "\n",
        "You are given part of the training loop below. At each epoch (the outer `for` loop), the model goes through the whole training data. At each step (the inner `for` loop), the model takes a mini-batch of training data, calculates the loss, and updates the weights and biases using the gradient computed on this mini-batch. After the whole training set is covered once, the epoch ends and the next epoch starts. Complete the missing parts below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDqY7eE54KqR"
      },
      "source": [
        "avg_loss = []\n",
        "val_losses = []\n",
        "\n",
        "################### Your code goes here ################\n",
        "# TODO: initialize weights and biases using the initialize_weights_and_biases function\n",
        "#       you implemented above.\n",
        "weights, biases = \n",
        "########################################################\n",
        "\n",
        "weight_bias_memory = []\n",
        "\n",
        "num_steps = len(x_train) // batch_size\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "\n",
        "    weight_bias_memory.append([float(weights), float(biases)])\n",
        "    losses = []\n",
        "\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x = x_train[step*batch_size: (step+1)*batch_size] \n",
        "        batch_y = y_train[step*batch_size: (step+1)*batch_size]\n",
        "        \n",
        "        ################### Your code goes here ################\n",
        "        # TODO: Calculate the predictions of the model on batch_x\n",
        "        y_hat = \n",
        "\n",
        "        # TODO: Find the mean squared error for y_hat and batch_y\n",
        "        loss = \n",
        "\n",
        "        # TODO: Update the parameters. \n",
        "        weights, biases = \n",
        "        ########################################################\n",
        "        \n",
        "        losses.append(np.sqrt(loss))\n",
        "\n",
        "    avg_loss.append(np.mean(losses))\n",
        "            \n",
        "    y_hat = forward_pass(x_test, weights, biases)\n",
        "    val_loss = np.sqrt(mean_squared_error(y_hat, y_test))\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    print(\"Epoch %i,   Validation loss %f, Training loss %f\" %(epoch, val_loss, np.mean(losses)))\n",
        "\n",
        "plt.plot(val_losses, label = \"Validation loss\")\n",
        "plt.plot(avg_loss, label = \"Training loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.title(\"Learning rate =\" + str(learning_rate) + \" Batch size =\" + str(batch_size))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbseUrJ6dw9L"
      },
      "source": [
        "Let's plot the progression of the regression lines. The following function plots the lines corresponding to the weights after every 10 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cbteAZBLO1B"
      },
      "source": [
        "project_2_utils.plot_regression_lines(weight_bias_memory, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxsJT-TxeD8E"
      },
      "source": [
        "Here we plot the line learned by the algorithm along with the test data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkQpQ-oeQhxU"
      },
      "source": [
        "project_2_utils.plot_regression_line(float(weights), float(biases))\n",
        "project_2_utils.plot_regression_data(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU_SN1MpeLHw"
      },
      "source": [
        "Values of the learned weights and biases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H611D7TGuZx_"
      },
      "source": [
        "print('weights = ', weights)\n",
        "print('biases = ', biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNCgdwZpeYZe"
      },
      "source": [
        "The following function plots the contours of the mean squared error function over the $(w,b)$ parameter space. The red dots are plotting the weight and bias values at each epoch. As seen from the plot, the pair $(w,b)$ approaches the center of the plot, corresponding to the parameters for the global minimum value of the error function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXFGwUYjUUGj"
      },
      "source": [
        "project_2_utils.plot_gradient_descent_progression(weight_bias_memory, x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiAopPeZfMsu"
      },
      "source": [
        "# Exercise 7 (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3m1vbFxSqqP"
      },
      "source": [
        "The error function for the linear regression problem is a quadratic function of the parameters and so it has a global minimum and no local minimums. Hence it is possible to find an explicit solution for the optimization problem. To this end, we first extend the data matrix `X` by adding a column of ones corresponding to bias parameter. So the function $f_{w,b}$ can be written as follows:\n",
        "$$\n",
        "\\begin{eqnarray*}\n",
        "\\hat{y} & = & f_{w,b}(X) \\\\ \n",
        "& = & X \\cdot w + b  \\\\\n",
        "& = & \\begin{bmatrix}\n",
        "x^{(1)}_1 & \\ldots & x^{(1)}_n & 1 \\\\ \\vdots & \\ddots & \\vdots & \\vdots \\\\ x^{(m)}_1 & \\ldots & x^{(m)}_n & 1\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "w_1 \\\\ \\vdots \\\\ w_n \\\\ b\n",
        "\\end{bmatrix} \\\\\n",
        "& = & \\tilde{X} \\cdot \\tilde{w}\n",
        "\\end{eqnarray*}\n",
        "$$\n",
        "For the linear system of equations $y = \\tilde{X} \\cdot \\tilde{w}$, the least square solution can be found as follows:\n",
        "$$\n",
        "\\tilde{w} = \\tilde{X}^+ \\cdot y\n",
        "$$\n",
        "where $\\tilde{X}^+ = (\\tilde{X}^T \\tilde{X})^{-1} \\tilde{X}^T$ (i.e., the [Moore-Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of the data matrix $\\tilde{X}$). \n",
        "\n",
        "Implement the following function which takes the data matrix `X` and the labels `y` as its arguments and returns the least squares solution using the psuedo-inverse described above. \n",
        "\n",
        "Hint: You can use `np.append` to create $\\tilde{X}$ and [np.linalg.pinv](https://numpy.org/doc/stable/reference/generated/numpy.linalg.pinv.html) to find the pseudo-inverso of $\\tilde{X}$. Read the documentation of `np.linalg.pinv` function which explains how the singular value decomposition is used to compute the pseudo-inverse. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5di_JBuqWIrB"
      },
      "source": [
        "def explicit_solution(X, y):\n",
        "\n",
        "    ################### Your code goes here ################\n",
        "\n",
        "\n",
        "    \n",
        "    ########################################################\n",
        "\n",
        "    return w_tilde\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_4urD9GV3iX"
      },
      "source": [
        "project_2_tests.test_explicit_solution(explicit_solution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46kWPwDLnVAx"
      },
      "source": [
        "Now run this function with training data as input and compare the parameter values you get with the resulting values of the gradient descent algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY6sVkMxV2KL"
      },
      "source": [
        "explicit_solution(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHpa9kysd3B4"
      },
      "source": [
        "# Exercise 8 (10 points)\n",
        "\n",
        "Let's try the algorithm you implemented on a real data set. We'll use the [auto-mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg) dataset from the [UCI Machine Learning repository](https://archive.ics.uci.edu/ml/index.php). The goal is to predict MPG (miles per gallon) from certain features of a car. The provided features are: 'Cylinders','Displacement','Horsepower','Weight', 'Acceleration', 'Model Year', 'Origin'. \n",
        "\n",
        "The last feature 'Origin' is a categorical feature which takes three possible values 'USA', 'Europe' or 'Japan'. One way to turn this kind of categorical feature into numbers is called `One Hot Encoding`: A new binary feature is added to the data corresponding to each category. In this case the 'Origin' feature column is replaced by three feature columns for which only one of these three feature columns is 1 and the rest are 0. We'll see what this looks like in a minute. \n",
        "\n",
        "Run the following code cells to load the training and the test dataset and take a look at the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6C8ph2JLw1i"
      },
      "source": [
        "df, x_train, y_train, x_test, y_test = project_2_utils.load_auto_mpg_data() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPmU_57PRgfc"
      },
      "source": [
        "Let's check out the dataset. Notice the one hot encoding of the 'Origin' feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zctv0b7GgKBx"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH_m5JfbhQxF"
      },
      "source": [
        "The dimensions of the data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YJqVbgohL83"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3JmrJTzgmDu"
      },
      "source": [
        "Now it is time to train the linear regression model on this dataset. \n",
        "\n",
        "**Hyperparameters**\n",
        "\n",
        "Set hyperparameters `batch_size` and `learning_rate`. Try different values of these parameters to get a validation loss less than 3.5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghNJwDQJEQgs"
      },
      "source": [
        "################### Your code goes here ################\n",
        "batch_size = \n",
        "learning_rate = \n",
        "########################################################\n",
        "\n",
        "# Do not change the epochs\n",
        "epochs = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z6h6WaHg1r5"
      },
      "source": [
        "**Train**\n",
        "\n",
        "As you did above for the artificial data set, fill in the required parts in the training loop below to train a linear regression model on the Auto-MPG dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iej90QjGDxhE"
      },
      "source": [
        "avg_loss = []\n",
        "val_losses = []\n",
        "\n",
        "################### Your code goes here ################\n",
        "# TODO: initialize weights and biases using the initialize_weights_and_biases\n",
        "#       you implemented above.\n",
        "weights, biases = \n",
        "########################################################\n",
        "\n",
        "num_steps = len(x_train) // batch_size\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "\n",
        "    losses = []\n",
        "\n",
        "    for step in range(0, num_steps):\n",
        "        batch_x = x_train[step*batch_size: (step+1)*batch_size] \n",
        "        batch_y = y_train[step*batch_size: (step+1)*batch_size]\n",
        "        \n",
        "        ################### Your code goes here ################\n",
        "        # TODO: Calculate the predictions of the model on batch_x\n",
        "        y_hat = \n",
        "\n",
        "        # TODO: Find the mean squared error for y_hat and batch_y and append the  \n",
        "        #       result to the losses list. \n",
        "        loss = \n",
        "\n",
        "        # TODO: Update the parameters. \n",
        "        weights, biases = \n",
        "        ########################################################\n",
        "        losses.append(np.sqrt(loss))\n",
        "\n",
        "    avg_loss.append(np.mean(losses))\n",
        "            \n",
        "    y_hat = forward_pass(x_test, weights, biases)\n",
        "    val_loss = np.sqrt(mean_squared_error(y_hat, y_test))\n",
        "    val_losses.append(val_loss)\n",
        "    \n",
        "    print(\"Epoch %i,   Validation loss %f, Training loss %f\" %(epoch, val_loss, np.mean(losses)))\n",
        "\n",
        "plt.plot(val_losses, label = \"Validation loss\")\n",
        "plt.plot(avg_loss, label = \"Training loss\")\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend()\n",
        "plt.title(\"Learning rate =\" + str(learning_rate) + \" Batch size =\" + str(batch_size))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gi1XmfSMEClB"
      },
      "source": [
        "weights, biases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeUBHz1DhrCW"
      },
      "source": [
        "Let's see how the model is doing by comparing the predictions of the model and the labels for the first 10 data points in the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5uPoDyJIQ1U"
      },
      "source": [
        "forward_pass(x_test[:10], weights, biases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwTCenivM45j"
      },
      "source": [
        "y_test[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCXbHaRQM59r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}